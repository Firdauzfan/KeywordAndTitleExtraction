{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Topic Modelling Klasifikasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv,numpy,pandas,nltk\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_topic</th>\n",
       "      <th>article_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93205794</td>\n",
       "      <td>Internasional</td>\n",
       "      <td>Kepolisian Inggris tengah memburu pelaku yang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93186698</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>Seluruh layanan transaksi di jalan tol akan m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93191463</td>\n",
       "      <td>Teknologi</td>\n",
       "      <td>\\nHari ini, Rabu (23/8), ternyata menjadi har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93219292</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>Saat ini Indonesia hanya memiliki cadangan ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>343106</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>Hari ini, Selasa (1/8), pedangdut Ridho Rhoma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  article_topic  \\\n",
       "0    93205794  Internasional   \n",
       "1    93186698        Ekonomi   \n",
       "2    93191463      Teknologi   \n",
       "3    93219292        Ekonomi   \n",
       "4      343106        Hiburan   \n",
       "\n",
       "                                     article_content  \n",
       "0   Kepolisian Inggris tengah memburu pelaku yang...  \n",
       "1   Seluruh layanan transaksi di jalan tol akan m...  \n",
       "2   \\nHari ini, Rabu (23/8), ternyata menjadi har...  \n",
       "3   Saat ini Indonesia hanya memiliki cadangan ba...  \n",
       "4   Hari ini, Selasa (1/8), pedangdut Ridho Rhoma...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baca data CSV dengan pandas\n",
    "df = pandas.read_csv('../data/ds_asg_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_topic\n",
      "Bisnis             25\n",
      "Bojonegoro        260\n",
      "Ekonomi          1762\n",
      "Haji             1497\n",
      "Health            131\n",
      "Hiburan          1466\n",
      "Horor              50\n",
      "Hukum              85\n",
      "Internasional     741\n",
      "Jakarta            12\n",
      "K-Pop              61\n",
      "KPK                37\n",
      "Kesehatan         195\n",
      "Keuangan           14\n",
      "Lifestyle         572\n",
      "MotoGP             35\n",
      "Obat-obatan        58\n",
      "Otomotif          174\n",
      "Pendidikan         70\n",
      "Personal           81\n",
      "Pilgub Jatim       25\n",
      "Politik           104\n",
      "Regional           35\n",
      "Sains             174\n",
      "Sejarah            70\n",
      "Sepak Bola       1184\n",
      "Sports            435\n",
      "Teknologi         571\n",
      "Travel             76\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#View berdasar topic\n",
    "groupby= df.groupby('article_topic').size()\n",
    "print (groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View berdasarkan topik dan didapatkan 29 topik dengan topik Ekonomi mempunyai proporsi paling banyak, dan dapat dilihat data tersebut tidak proporsional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicek dahulu apakah data terdapat missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "      article_id  article_topic  article_content\n",
      "0          False          False            False\n",
      "1          False          False            False\n",
      "2          False          False            False\n",
      "3          False          False            False\n",
      "4          False          False            False\n",
      "5          False          False            False\n",
      "6          False          False            False\n",
      "7          False          False            False\n",
      "8          False          False            False\n",
      "9          False          False            False\n",
      "10         False          False            False\n",
      "11         False          False            False\n",
      "12         False          False            False\n",
      "13         False          False            False\n",
      "14         False          False            False\n",
      "15         False          False            False\n",
      "16         False          False            False\n",
      "17         False          False            False\n",
      "18         False          False            False\n",
      "19         False          False            False\n",
      "20         False          False            False\n",
      "21         False          False            False\n",
      "22         False          False            False\n",
      "23         False          False            False\n",
      "24         False          False            False\n",
      "25         False          False            False\n",
      "26         False          False            False\n",
      "27         False          False            False\n",
      "28         False          False            False\n",
      "29         False          False            False\n",
      "...          ...            ...              ...\n",
      "9970       False          False            False\n",
      "9971       False          False            False\n",
      "9972       False          False            False\n",
      "9973       False          False            False\n",
      "9974       False          False            False\n",
      "9975       False          False            False\n",
      "9976       False          False            False\n",
      "9977       False          False            False\n",
      "9978       False          False            False\n",
      "9979       False          False            False\n",
      "9980       False          False            False\n",
      "9981       False          False            False\n",
      "9982       False          False            False\n",
      "9983       False          False            False\n",
      "9984       False          False            False\n",
      "9985       False          False            False\n",
      "9986       False          False            False\n",
      "9987       False          False            False\n",
      "9988       False          False            False\n",
      "9989       False          False            False\n",
      "9990       False          False            False\n",
      "9991       False          False            False\n",
      "9992       False          False            False\n",
      "9993       False          False            False\n",
      "9994       False          False            False\n",
      "9995       False          False            False\n",
      "9996       False          False            False\n",
      "9997       False          False            False\n",
      "9998       False          False            False\n",
      "9999       False          False            False\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "article_id          0\n",
      "article_topic       0\n",
      "article_content    36\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Cek data dan normalisasi\n",
    "#cek bila ada yang kosong\n",
    "cek_bentuk= df['article_content'].shape\n",
    "cek_null=df.isnull()\n",
    "cek_jumlah_null=df.isnull().sum()\n",
    "\n",
    "print (cek_bentuk)\n",
    "print (cek_null)\n",
    "print (cek_jumlah_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah cek data apakah ada data yang null atau tidak, karena ada data yang null/missing value maka perlu dinormalisasi terlebih dahulu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita hapus data yang terdapat missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9964,)\n",
      "      article_id  article_topic  article_content\n",
      "0          False          False            False\n",
      "1          False          False            False\n",
      "2          False          False            False\n",
      "3          False          False            False\n",
      "4          False          False            False\n",
      "5          False          False            False\n",
      "6          False          False            False\n",
      "7          False          False            False\n",
      "8          False          False            False\n",
      "9          False          False            False\n",
      "10         False          False            False\n",
      "11         False          False            False\n",
      "12         False          False            False\n",
      "13         False          False            False\n",
      "14         False          False            False\n",
      "15         False          False            False\n",
      "16         False          False            False\n",
      "17         False          False            False\n",
      "18         False          False            False\n",
      "19         False          False            False\n",
      "20         False          False            False\n",
      "21         False          False            False\n",
      "22         False          False            False\n",
      "23         False          False            False\n",
      "24         False          False            False\n",
      "25         False          False            False\n",
      "26         False          False            False\n",
      "27         False          False            False\n",
      "28         False          False            False\n",
      "29         False          False            False\n",
      "...          ...            ...              ...\n",
      "9970       False          False            False\n",
      "9971       False          False            False\n",
      "9972       False          False            False\n",
      "9973       False          False            False\n",
      "9974       False          False            False\n",
      "9975       False          False            False\n",
      "9976       False          False            False\n",
      "9977       False          False            False\n",
      "9978       False          False            False\n",
      "9979       False          False            False\n",
      "9980       False          False            False\n",
      "9981       False          False            False\n",
      "9982       False          False            False\n",
      "9983       False          False            False\n",
      "9984       False          False            False\n",
      "9985       False          False            False\n",
      "9986       False          False            False\n",
      "9987       False          False            False\n",
      "9988       False          False            False\n",
      "9989       False          False            False\n",
      "9990       False          False            False\n",
      "9991       False          False            False\n",
      "9992       False          False            False\n",
      "9993       False          False            False\n",
      "9994       False          False            False\n",
      "9995       False          False            False\n",
      "9996       False          False            False\n",
      "9997       False          False            False\n",
      "9998       False          False            False\n",
      "9999       False          False            False\n",
      "\n",
      "[9964 rows x 3 columns]\n",
      "article_id         0\n",
      "article_topic      0\n",
      "article_content    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Drop/Hapus data yang kosong\n",
    "modifiedData = df.dropna()\n",
    "\n",
    "cek_bentuk= modifiedData['article_content'].shape\n",
    "cek_null=modifiedData.isnull()\n",
    "cek_jumlah_null=modifiedData.isnull().sum()\n",
    "\n",
    "#Save Data yang sudah dimodifikasi ->opsional\n",
    "modifiedData.to_csv('modifiedData.csv',index=False)\n",
    "\n",
    "print (cek_bentuk)\n",
    "print (cek_null)\n",
    "print (cek_jumlah_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita Hapus data yang missing value, maka 36 data yang kosong tersebut dihilangkan. Lalu kita load lagi data yang telah dinormalisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baca data CSV dengan pandas\n",
    "df = pandas.read_csv('modifiedData.csv')\n",
    "Id=df['article_id']\n",
    "Con= df['article_content']\n",
    "Topic = df['article_topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah itu dilakukan tahap tokenisasi yaitu memecah kalimat menjadi kata-kata yang dilakukan untuk menjadikan sebuah kalimat menjadi lebih bermakna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pisahkan kata/tokenize data\n",
    "\n",
    "PK= [nltk.word_tokenize(PisahKata) for PisahKata in df['article_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah itu masuk ketahap stopword removal/filtrasi yaitu filtrasi adalah tahap mengambil kata-kata penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = ['ada','adalah','adanya','adapun','agak','agaknya','agar','akan','akankah',\n",
    "                 'akhir','akhiri','akhirnya','aku','akulah','amat','amatlah','anda','andalah',\n",
    "                 'antar','antara','antaranya','apa','apaan','apabila','apakah','apalagi','apatah',\n",
    "                 'artinya','asal','asalkan','atas','atau','ataukah','ataupun','awal','awalnya','bagai',\n",
    "                 'bagaikan','bagaimana','bagaimanakah','bagaimanapun','bagi','bagian','bahkan','bahwa',\n",
    "                 'bahwasanya','baik','bakal','bakalan','balik','banyak','bapak','baru','bawah','beberapa',\n",
    "                 'begini','beginian','beginikah','beginilah','begitu','begitukah','begitulah','begitupun',\n",
    "                 'bekerja','belakang','belakangan','belum','belumlah','benar','benarkah','benarlah','berada',\n",
    "                 'berakhir','berakhirlah','berakhirnya','berapa','berapakah','berapalah','berapapun','berarti',\n",
    "                 'berawal','berbagai','berdatangan','beri','berikan','berikut','berikutnya','berjumlah','berkali-kali',\n",
    "                 'berkata','berkehendak','berkeinginan','berkenaan','berlainan','berlalu','berlangsung','berlebihan','bermacam',\n",
    "                 'bermacam-macam','bermaksud','bermula','bersama','bersama-sama','bersiap','bersiap-siap','bertanya','bertanya-tanya','berturut','berturut-turut','bertutur','berujar','berupa','besar','betul','betulkah','biasa','biasanya','bila','bilakah','bisa','bisakah','boleh','bolehkah','bolehlah','buat','bukan','bukankah','bukanlah','bukannya','bulan','bung','cara','caranya','cukup','cukupkah','cukuplah','cuma','dahulu','dalam','dan','dapat','dari','daripada','datang','dekat','demi','demikian','demikianlah','dengan','depan','di','dia','diakhiri','diakhirinya','dialah','diantara','diantaranya','diberi','diberikan','diberikannya','dibuat','dibuatnya','didapat','didatangkan','digunakan','diibaratkan','diibaratkannya','diingat','diingatkan','diinginkan','dijawab','dijelaskan','dijelaskannya','dikarenakan','dikatakan','dikatakannya','dikerjakan','diketahui','diketahuinya','dikira','dilakukan','dilalui','dilihat','dimaksud','dimaksudkan','dimaksudkannya','dimaksudnya','diminta','dimintai','dimisalkan','dimulai','dimulailah','dimulainya','dimungkinkan','dini','dipastikan','diperbuat','diperbuatnya','dipergunakan','diperkirakan','diperlihatkan','diperlukan','diperlukannya','dipersoalkan','dipertanyakan','dipunyai','diri','dirinya','disampaikan','disebut','disebutkan','disebutkannya','disini','disinilah','ditambahkan','ditandaskan','ditanya','ditanyai','ditanyakan','ditegaskan','ditujukan','ditunjuk','ditunjuki','ditunjukkan','ditunjukkannya','ditunjuknya','dituturkan','dituturkannya','diucapkan','diucapkannya','diungkapkan','dong','dua','dulu','empat','enggak','enggaknya','entah','entahlah','guna','gunakan','hal','hampir','hanya','hanyalah','hari','harus','haruslah','harusnya','hendak','hendaklah','hendaknya','hingga','ia','ialah','ibarat','ibaratkan','ibaratnya','ibu','ikut','ingat','ingat-ingat','ingin','inginkah','inginkan','ini','inikah','inilah','itu','itukah','itulah','jadi','jadilah','jadinya','jangan','jangankan','janganlah','jauh','jawab','jawaban','jawabnya','jelas','jelaskan','jelaslah','jelasnya','jika','jikalau','juga','jumlah','jumlahnya','justru','kala','kalau','kalaulah','kalaupun','kalian','kami','kamilah','kamu','kamulah','kan','kapan','kapankah','kapanpun','karena','karenanya','kasus','kata','katakan','katakanlah','katanya','ke','keadaan','kebetulan','kecil','kedua','keduanya','keinginan','kelamaan','kelihatan','kelihatannya','kelima','keluar','kembali','kemudian','kemungkinan','kemungkinannya','kenapa','kepada','kepadanya','kesampaian','keseluruhan','keseluruhannya','keterlaluan','ketika','khususnya','kini','kinilah','kira','kira-kira','kiranya','kita','kitalah','kok','kurang','lagi','lagian','lah','lain','lainnya','lalu','lama','lamanya','lanjut','lanjutnya','lebih','lewat','lima','luar','macam','maka','makanya','makin','malah','malahan','mampu','mampukah','mana','manakala','manalagi','masa','masalah','masalahnya','masih','masihkah','masing','masing-masing','mau','maupun','melainkan','melakukan','melalui','melihat','melihatnya','memang','memastikan','memberi','memberikan','membuat','memerlukan','memihak','meminta','memintakan','memisalkan','memperbuat','mempergunakan','memperkirakan','memperlihatkan','mempersiapkan','mempersoalkan','mempertanyakan','mempunyai','memulai','memungkinkan','menaiki','menambahkan','menandaskan','menanti','menanti-nanti','menantikan','menanya','menanyai','menanyakan','mendapat','mendapatkan','mendatang','mendatangi','mendatangkan','menegaskan','mengakhiri','mengapa','mengatakan','mengatakannya','mengenai','mengerjakan','mengetahui','menggunakan','menghendaki','mengibaratkan','mengibaratkannya','mengingat','mengingatkan','menginginkan','mengira','mengucapkan','mengucapkannya','mengungkapkan','menjadi','menjawab','menjelaskan','menuju','menunjuk','menunjuki','menunjukkan','menunjuknya','menurut','menuturkan','menyampaikan','menyangkut','menyatakan','menyebutkan','menyeluruh','menyiapkan','merasa','mereka','merekalah','merupakan','meski','meskipun','meyakini','meyakinkan','minta','mirip','misal','misalkan','misalnya','mula','mulai','mulailah','mulanya','mungkin','mungkinkah','nah','naik','namun','nanti','nantinya','nyaris','nyatanya','oleh','olehnya','pada','padahal','padanya','pak','paling','panjang','pantas','para','pasti','pastilah','penting','pentingnya','per','percuma','perlu','perlukah','perlunya','pernah','persoalan','pertama','pertama-tama','pertanyaan','pertanyakan','pihak','pihaknya','pukul','pula','pun','punya','rasa','rasanya','rata','rupanya','saat','saatnya','saja','sajalah','saling','sama','sama-sama','sambil','sampai','sampai-sampai','sampaikan','sana','sangat','sangatlah','satu','saya','sayalah','se','sebab','sebabnya','sebagai','sebagaimana','sebagainya','sebagian','sebaik','sebaik-baiknya','sebaiknya','sebaliknya','sebanyak','sebegini','sebegitu','sebelum','sebelumnya','sebenarnya','seberapa','sebesar','sebetulnya','sebisanya','sebuah','sebut','sebutlah','sebutnya','secara','secukupnya','sedang','sedangkan','sedemikian','sedikit','sedikitnya','seenaknya','segala','segalanya','segera','seharusnya','sehingga','seingat','sejak','sejauh','sejenak','sejumlah','sekadar','sekadarnya','sekali','sekali-kali','sekalian','sekaligus','sekalipun','sekarang','sekarang','sekecil','seketika','sekiranya','sekitar','sekitarnya','sekurang-kurangnya','sekurangnya','sela','selain','selaku','selalu','selama','selama-lamanya','selamanya','selanjutnya','seluruh','seluruhnya','semacam','semakin','semampu','semampunya','semasa','semasih','semata','semata-mata','semaunya','sementara','semisal','semisalnya','sempat','semua','semuanya','semula','sendiri','sendirian','sendirinya','seolah','seolah-olah','seorang','sepanjang','sepantasnya','sepantasnyalah','seperlunya','seperti','sepertinya','sepihak','sering','seringnya','serta','serupa','sesaat','sesama','sesampai','sesegera','sesekali','seseorang','sesuatu','sesuatunya','sesudah','sesudahnya','setelah','setempat','setengah','seterusnya','setiap','setiba','setibanya','setidak-tidaknya','setidaknya','setinggi','seusai','sewaktu','siap','siapa','siapakah','siapapun','sini','sinilah','soal','soalnya','suatu','sudah','sudahkah','sudahlah','supaya','tadi','tadinya','tahu','tahun','tak','tambah','tambahnya','tampak','tampaknya','tandas','tandasnya','tanpa','tanya','tanyakan','tanyanya','tapi','tegas','tegasnya','telah','tempat','tengah','tentang','tentu','tentulah','tentunya','tepat','terakhir','terasa','terbanyak','terdahulu','terdapat','terdiri','terhadap','terhadapnya','teringat','teringat-ingat','terjadi','terjadilah','terjadinya','terkira','terlalu','terlebih','terlihat','termasuk','ternyata','tersampaikan','tersebut','tersebutlah','tertentu','tertuju','terus','terutama','tetap','tetapi','tiap','tiba','tiba-tiba','tidak','tidakkah','tidaklah','tiga','tinggi','toh','tunjuk','turut','tutur','tuturnya','ucap','ucapnya','ujar','ujarnya','umum','umumnya','ungkap','ungkapnya','untuk','usah','usai','waduh','wah','wahai','waktu','waktunya','walau','walaupun','wong','yaitu','yakin','yakni','yang']\n",
    "\n",
    "sw=[]\n",
    "\n",
    "for conten in PK:\n",
    "    kata= list(filter(lambda x: x not in stopword, conten))\n",
    "    sw.append(kata)\n",
    "\n",
    "df['stopword'] = sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah menghilangkan kata-kata yang tidak penting akan tersisa kata-kata yang penting saja pada data. lalu kita ubah data dari list ke string untuk proses selanjutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ubah list ke str\n",
    "string = map(' '.join, sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu setelah itu masuk ketahap feature extraction dimana kata-kata pada data diubah menjadi vector model menggunakan term frequency(tf) dan term frequency-inverse document frequency(tf-idf). tf(countvector/raw count) cocok digunakan untuk algoritma naive bayes karena naive bayes menggunakan probabilitas dalam prediksinya, dan tf-idf cocok untuk algoritma SVM karena TF-IDF mengukur relevansi, bukan frekuensi. Artinya, jumlah kata digantikan dengan nilai TF-IDF di seluruh dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 41690)\t0.31868739085265324\n",
      "  (0, 35543)\t0.22343718754799582\n",
      "  (0, 51457)\t0.086947941551807\n",
      "  (0, 63715)\t0.12045952256960622\n",
      "  (0, 50680)\t0.10840495775857466\n",
      "  (0, 14912)\t0.24480796826896137\n",
      "  (0, 79938)\t0.14630997870732254\n",
      "  (0, 47412)\t0.282032599471728\n",
      "  (0, 75492)\t0.1780299110284095\n",
      "  (0, 1900)\t0.09621245926144197\n",
      "  (0, 61618)\t0.11591891954000397\n",
      "  (0, 84615)\t0.15496105386747977\n",
      "  (0, 66447)\t0.12090168105148284\n",
      "  (0, 38301)\t0.044296161657492485\n",
      "  (0, 62267)\t0.051302804673997275\n",
      "  (0, 77779)\t0.0743893006264868\n",
      "  (0, 871)\t0.04186524312591781\n",
      "  (0, 54970)\t0.07703333079153188\n",
      "  (0, 8909)\t0.07607253273919327\n",
      "  (0, 43737)\t0.08601740271837825\n",
      "  (0, 49373)\t0.08719043687285524\n",
      "  (0, 72985)\t0.1251249224240002\n",
      "  (0, 22658)\t0.05435663377818881\n",
      "  (0, 85339)\t0.04420135341259307\n",
      "  (0, 31605)\t0.08176857179202043\n",
      "  :\t:\n",
      "  (9963, 9314)\t0.029212684520568473\n",
      "  (9963, 27774)\t0.03679745339177746\n",
      "  (9963, 54200)\t0.02939892541579596\n",
      "  (9963, 27484)\t0.033126867804607774\n",
      "  (9963, 2614)\t0.10672812719525913\n",
      "  (9963, 35360)\t0.03406107046357006\n",
      "  (9963, 61474)\t0.03490985051464225\n",
      "  (9963, 78699)\t0.030461561089234385\n",
      "  (9963, 88820)\t0.033126867804607774\n",
      "  (9963, 31334)\t0.032004161600793155\n",
      "  (9963, 8117)\t0.03728794889562019\n",
      "  (9963, 78397)\t0.0363537462366579\n",
      "  (9963, 78719)\t0.03357057495972733\n",
      "  (9963, 97)\t0.03728794889562019\n",
      "  (9963, 31535)\t0.13428229983890932\n",
      "  (9963, 86612)\t0.17788021199209855\n",
      "  (9963, 7854)\t0.0698197010292845\n",
      "  (9963, 74734)\t0.03845791846489341\n",
      "  (9963, 62410)\t0.040024331823827586\n",
      "  (9963, 89228)\t0.07567255243568095\n",
      "  (9963, 71755)\t0.11752665531826617\n",
      "  (9963, 75597)\t0.15383167385957364\n",
      "  (9963, 7853)\t0.04429003308194072\n",
      "  (9963, 82394)\t0.04429003308194072\n",
      "  (9963, 15492)\t0.04429003308194072\n"
     ]
    }
   ],
   "source": [
    "#CountVector/raw count\n",
    "count_vect = CountVectorizer()\n",
    "count = count_vect.fit_transform(string)\n",
    "\n",
    "#tf-idf\n",
    "tf_transformer = TfidfTransformer().fit(count)\n",
    "X_train_tf = tf_transformer.transform(count)\n",
    "print (X_train_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil diatas adalah hasil pembobotan tiap kata pada data, pembobotan menghitung seberapa pentingnya sebuah kata dalam sebuah\n",
    "dokumen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and Evaluasi pada data sendiri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilakukan prediksi dengan langsung melakukan tes pada data train itu sendiri, dalam model kali ini menggunakan algoritma naive bayes dan juga support vector machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita coba dengan Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9139903653151344\n",
      "(row=expected, col=predicted)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Bisnis       1.00      0.16      0.28        25\n",
      "   Bojonegoro       0.95      0.95      0.95       260\n",
      "      Ekonomi       0.92      0.99      0.95      1760\n",
      "         Haji       0.99      0.99      0.99      1497\n",
      "       Health       0.84      0.83      0.84       131\n",
      "      Hiburan       0.91      0.98      0.94      1448\n",
      "        Horor       1.00      0.68      0.81        50\n",
      "        Hukum       0.96      0.86      0.91        85\n",
      "Internasional       0.89      0.97      0.93       739\n",
      "      Jakarta       0.00      0.00      0.00        12\n",
      "        K-Pop       1.00      0.28      0.44        61\n",
      "          KPK       1.00      0.76      0.86        37\n",
      "    Kesehatan       0.80      0.93      0.86       195\n",
      "     Keuangan       1.00      0.07      0.13        14\n",
      "    Lifestyle       0.84      0.94      0.88       568\n",
      "       MotoGP       1.00      0.31      0.48        35\n",
      "  Obat-obatan       0.96      0.47      0.63        58\n",
      "     Otomotif       1.00      0.89      0.94       173\n",
      "   Pendidikan       1.00      0.80      0.89        70\n",
      "     Personal       0.98      0.51      0.67        81\n",
      " Pilgub Jatim       0.95      0.80      0.87        25\n",
      "      Politik       0.93      0.81      0.86       103\n",
      "     Regional       1.00      0.29      0.44        35\n",
      "        Sains       1.00      0.33      0.49       174\n",
      "      Sejarah       1.00      0.77      0.87        70\n",
      "   Sepak Bola       0.86      0.99      0.92      1180\n",
      "       Sports       0.88      0.56      0.69       435\n",
      "    Teknologi       0.94      0.94      0.94       567\n",
      "       Travel       1.00      0.67      0.80        76\n",
      "\n",
      "  avg / total       0.92      0.91      0.90      9964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Predictor Naive Bayes & Support Vector Machine()\n",
    "topic_modelling = MultinomialNB().fit(count, Topic)\n",
    "\n",
    "prediksi_semua = topic_modelling.predict(count)\n",
    "\n",
    "print ('accuracy', accuracy_score(Topic, prediksi_semua))\n",
    "print ('(row=expected, col=predicted)')\n",
    "print (classification_report(Topic, prediksi_semua))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil dengan Naive Bayes dengan tingkat akurasi 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kemudian coba dengan SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9956844640706544\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Bisnis       1.00      0.96      0.98        25\n",
      "   Bojonegoro       1.00      1.00      1.00       260\n",
      "      Ekonomi       1.00      1.00      1.00      1760\n",
      "         Haji       1.00      1.00      1.00      1497\n",
      "       Health       0.99      0.98      0.98       131\n",
      "      Hiburan       0.99      1.00      0.99      1448\n",
      "        Horor       1.00      1.00      1.00        50\n",
      "        Hukum       1.00      1.00      1.00        85\n",
      "Internasional       1.00      1.00      1.00       739\n",
      "      Jakarta       1.00      1.00      1.00        12\n",
      "        K-Pop       1.00      0.92      0.96        61\n",
      "          KPK       1.00      1.00      1.00        37\n",
      "    Kesehatan       0.98      0.97      0.98       195\n",
      "     Keuangan       1.00      1.00      1.00        14\n",
      "    Lifestyle       1.00      1.00      1.00       568\n",
      "       MotoGP       1.00      1.00      1.00        35\n",
      "  Obat-obatan       0.98      0.98      0.98        58\n",
      "     Otomotif       1.00      1.00      1.00       173\n",
      "   Pendidikan       1.00      1.00      1.00        70\n",
      "     Personal       1.00      1.00      1.00        81\n",
      " Pilgub Jatim       1.00      1.00      1.00        25\n",
      "      Politik       0.99      0.97      0.98       103\n",
      "     Regional       1.00      1.00      1.00        35\n",
      "        Sains       1.00      1.00      1.00       174\n",
      "      Sejarah       1.00      1.00      1.00        70\n",
      "   Sepak Bola       0.98      1.00      0.99      1180\n",
      "       Sports       1.00      0.95      0.98       435\n",
      "    Teknologi       1.00      1.00      1.00       567\n",
      "       Travel       1.00      1.00      1.00        76\n",
      "\n",
      "  avg / total       1.00      1.00      1.00      9964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predictor Naive Bayes BernoulliNB() & SVM LinearSVC()\n",
    "topic_modelling = LinearSVC().fit(X_train_tf, Topic)\n",
    "\n",
    "prediksi_semua = topic_modelling.predict(X_train_tf)\n",
    "\n",
    "print ('accuracy', accuracy_score(Topic, prediksi_semua))\n",
    "\n",
    "print (classification_report(Topic, prediksi_semua))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil dengan SVM dengan tingkat akurasi 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada evaluasi diatas dilakukan klasifikasi pada data yang sama yang digunakan untuk data latih. Hasil dari klasifikasi tersebut tidaklah valid karena model yang diklasifikasikan akan terjadi overfitting dimana kondisi dimana sebuah model klasifikasi dapat melakukan klasifikasi yang sangat baik terhadap set data latih namun sangat buruk saat melakukan klasifikasi data yang baru atau yang belum pernah ada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and Evaluasi dengan Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maka dari itu agar tidak terjadi overfitting klasifikasi dilakukan dengan data yang berbeda dari data latih, salah satu caranya adalah dengan K-Fold Cross Validation. Dengan K-Fold Cross Validation data latih tersebut dibagi menjadi data latih dan data tes oleh sistem sehingga data yang digunakan dalam klasifikasi merupakan data baru dan dapat terhindar dari overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba dengan Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8343254  0.8278607  0.83964143 0.81536926 0.844      0.83282981\n",
      " 0.84072581 0.84327604 0.84584178 0.84162437]\n",
      "0.8365494591067536 0.008847254962295797\n"
     ]
    }
   ],
   "source": [
    "#Performing klasifikasi dgn cross validation\n",
    "#Naive Bayes Classifier & SVM\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "scores = cross_val_score(model,  \n",
    "                         count,  \n",
    "                         Topic,  \n",
    "                         cv=StratifiedKFold(Topic, n_folds=10),    \n",
    "                         scoring='f1_micro',  \n",
    "                         n_jobs=-1,\n",
    "                         )\n",
    "                         \n",
    "print (scores)\n",
    "\n",
    "print (scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kemudian kita coba dengan SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88095238 0.88059701 0.9063745  0.88023952 0.9        0.88519637\n",
      " 0.87701613 0.88372093 0.89756592 0.89035533]\n",
      "0.8882018105585174 0.009432835421666515\n"
     ]
    }
   ],
   "source": [
    "#Performing klasifikasi dgn cross validation\n",
    "#Naive Bayes Classifier & SVM\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "scores = cross_val_score(model,  \n",
    "                         X_train_tf,  \n",
    "                         Topic,  \n",
    "                         cv=StratifiedKFold(Topic, n_folds=10),    \n",
    "                         scoring='f1_micro',  \n",
    "                         n_jobs=-1,\n",
    "                         )\n",
    "                         \n",
    "print (scores)\n",
    "\n",
    "print (scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didapatkan hasil bahwa perform algoritma Support Vector Machine lebih bagus dalam membuat prediksi pada data tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Works\n",
    "- Menggunakan fungsi regex untuk dapat mengklasifikasikan berdasarkan kata-kata kunci yang termasuk dalam topik yang mana\n",
    "- Menggunakan algoritma machine learning dan mecoba menerapkan deep learning dalam prediksi\n",
    "- Membuat normalisasi pada data yang belum proporsional\n",
    "- Mencoba menambah proses yang diperlukan pada preprocessing data seperti stemming, dan lain sebagainya\n",
    "- Mencoba Tuning parameter pada countvectorizer, tf-idf dan algoritma machine learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
